{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824caee7",
   "metadata": {},
   "source": [
    "# VibeCheck AI: Emotion Detection from Speech\n",
    "\n",
    "---  \n",
    "\n",
    "##  Objective\n",
    "We aim to build a deep learning system that classifies speech audio into **five emotions** — `happy`, `sad`, `angry`, `neutral`, and `excited` (if available in dataset) using the RAVDESS dataset. This notebook will cover audio preprocessing, feature extraction, data augmentation, model training (CNN, RNN, hybrid), and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a35987",
   "metadata": {},
   "source": [
    "## -- 1. Setup & Imports --\n",
    "* Install necessary libraries if required with `!pip xyz -q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "287ccceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, Shift\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af918a",
   "metadata": {},
   "source": [
    "## -- 2. Data Preparation (RAVDESS Subset Extractor) --\n",
    "* Assumes RAVDESS audio data is extracted locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb0db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"08\": \"excited\"  # excited mapped from 'surprised' if present\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36aa379",
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_root = \"data/Audio_Speech_Actors_01-24\"\n",
    "output_dir = \"data/filtered_ravdess\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for actor_dir in Path(ravdess_root).rglob(\"*.wav\"):\n",
    "    parts = actor_dir.name.split(\"-\")\n",
    "    emotion_id = parts[2]\n",
    "    if emotion_id in emotion_map:\n",
    "        label = emotion_map[emotion_id]\n",
    "        dest_dir = os.path.join(output_dir, label)\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(actor_dir, os.path.join(dest_dir, actor_dir.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73624983",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## -- 3. Audio Feature Extraction --\n",
    "* #### Extract MFCCs, Mel Spectrogram, Chroma, ZCR. Apply fixed-length padding/truncation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84173f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3  # 3 seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "\n",
    "\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
    "    y = librosa.util.fix_length(data=y, size=SAMPLES_PER_TRACK)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    features = np.vstack([mfcc, mel, chroma, zcr])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15261bfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset Construction\n",
    "# Load, label encode, normalize, and structure audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2787064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_dir(root_dir):\n",
    "    data, labels = [], []\n",
    "    for label in os.listdir(root_dir):\n",
    "        for file in os.listdir(os.path.join(root_dir, label)):\n",
    "            fpath = os.path.join(root_dir, label, file)\n",
    "            features = extract_features(fpath)\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "X, y = get_data_from_dir(output_dir)\n",
    "X = X.transpose(0, 2, 1)  # (samples, time, features)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "scaler = StandardScaler()\n",
    "X = np.array([scaler.fit_transform(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363518a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Augmentation\n",
    "# TimeStretch, PitchShift, and Shift are applied randomly to improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, TimeStretch, PitchShift, Shift\n",
    "\n",
    "augment = Compose([\n",
    "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "    Shift(min_shift=-0.2, max_shift=0.2, p=0.5)\n",
    "])\n",
    "# Applied only during training (in DataLoader or collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094e35c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b4a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dde37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b3cdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Classifier\n",
    "#Designed to extract spatial features from spectrograms.\n",
    "class CNNEmotion(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(32 * 47 * 46, 128) # This will be corrected in forward\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "        self._to_linear = None # Placeholder for the calculated flattened size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Check if the input is 3D (no channel dimension)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)  # Add the channel dimension\n",
    "\n",
    "        # Now x should be 4D: [B, C, T, F]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dynamically calculate the size of the flattened output\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.view(x.size(0), -1).shape[1]\n",
    "\n",
    "        x = x.view(x.size(0), -1) # Flatten for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# CNN Classifier\n",
    "# class CNNEmotion(nn.Module):\n",
    "#     def __init__(self, num_classes, dropout_rate=0.3):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "#         self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "#         self._to_linear = None\n",
    "#         # The actual size will be calculated in forward pass\n",
    "#         self.fc1 = nn.Linear(1, 128)  # Placeholder, will be updated in forward pass\n",
    "#         self.out = nn.Linear(128, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Check if the input is 3D (no channel dimension)\n",
    "#         if len(x.shape) == 3:\n",
    "#             x = x.unsqueeze(1)  # Add the channel dimension\n",
    "        \n",
    "#         # Now x should be 4D: [B, C, T, F]\n",
    "#         x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "#         x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "        \n",
    "#         # Dynamically calculate the size of the flattened output on first run\n",
    "#         if self._to_linear is None:\n",
    "#             self._to_linear = x.view(x.size(0), -1).shape[1]\n",
    "#             # Update fc1 input size\n",
    "#             self.fc1 = nn.Linear(self._to_linear, 128).to(x.device)\n",
    "        \n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = self.dropout(F.relu(self.fc1(x)))\n",
    "#         x = self.out(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f30ba53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM Model\n",
    "# Ideal for learning temporal patterns in MFCC sequences.\n",
    "# class LSTMEmotion(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         return self.fc(out[:, -1, :])\n",
    "\n",
    "class LSTMEmotion(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # Multiply by 2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Make sure input is 3D: [batch, seq, features]\n",
    "        if len(x.shape) == 4:  # If input is spectrogram [B, C, T, F]\n",
    "            batch_size, channels, time_steps, features = x.shape\n",
    "            x = x.view(batch_size, time_steps, channels * features)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        # Take the output from the last time step\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cc73246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Transformer-based model\n",
    "class TransformerEmotion(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_model=128, nhead=8, num_encoder_layers=4, \n",
    "                 dim_feedforward=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Project input features to d_model dimensions\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding - learnable\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1000, d_model))  # Max sequence length: 1000\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Handle input dimensionality\n",
    "        if len(x.shape) == 4:  # If input is spectrogram [B, C, T, F]\n",
    "            batch_size, channels, time_steps, features = x.shape\n",
    "            x = x.view(batch_size, time_steps, channels * features)\n",
    "        \n",
    "        # Project input to d_model dimensions\n",
    "        x = self.input_projection(x)  \n",
    "        \n",
    "        # Add positional encoding (take only what we need)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pos_encoder[:seq_len, :].unsqueeze(0)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Global average pooling over sequence length dimension\n",
    "        x = torch.mean(x, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff55f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single residual block for the ResNet.\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        stride (int): Stride for the first convolution layer. Default is 1.\n",
    "        dropout_rate (float): Dropout rate for the Dropout2d layer within the block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolutional layer in the block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True) # ReLU activation\n",
    "        # Dropout2d is used for feature maps (spatial dropout)\n",
    "        self.dropout_conv = nn.Dropout2d(p=dropout_rate) \n",
    "\n",
    "        # Second convolutional layer in the block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Downsample layer for the shortcut connection if dimensions or channels change\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x # Store the input for the shortcut connection\n",
    "\n",
    "        # Main path\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout_conv(out) # Apply dropout after the first activation\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Shortcut connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity # Add the shortcut connection\n",
    "        out = self.relu(out) # Final activation for the block\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNetEmotion(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual CNN model for emotion recognition.\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes.\n",
    "        dropout_rate (float): Dropout rate for FC layers and conv blocks. Default is 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.base_channels = 32 # Base number of channels for the first conv layers\n",
    "\n",
    "        # Initial convolution layer\n",
    "        # Input shape: [Batch, Channels, Time, Frequency], e.g., [B, 1, T, F]\n",
    "        self.conv1 = nn.Conv2d(1, self.base_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.base_channels)\n",
    "        self.relu = nn.ReLU(inplace=True) # Re-usable ReLU activation\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Halves Time and Frequency dimensions\n",
    "\n",
    "        # Residual blocks\n",
    "        # Block 1: Maintains dimensions and channel count (base_channels -> base_channels)\n",
    "        self.res_block1 = ResidualBlock(self.base_channels, self.base_channels, stride=1, dropout_rate=self.dropout_rate)\n",
    "        \n",
    "        # Block 2: Doubles channels and halves dimensions (base_channels -> base_channels * 2)\n",
    "        self.res_block2 = ResidualBlock(self.base_channels, self.base_channels * 2, stride=2, dropout_rate=self.dropout_rate)\n",
    "        self.current_channels = self.base_channels * 2 # Update current channel count\n",
    "\n",
    "        # (Optional) Add more blocks for a deeper network if needed:\n",
    "        # self.res_block3 = ResidualBlock(self.current_channels, self.current_channels, stride=1, dropout_rate=self.dropout_rate)\n",
    "        # self.res_block4 = ResidualBlock(self.current_channels, self.current_channels * 2, stride=2, dropout_rate=self.dropout_rate)\n",
    "        # self.current_channels *= 2\n",
    "\n",
    "        # Global average pooling reduces each feature map to a single value\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # Output: [B, current_channels, 1, 1]\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(self.current_channels, 128) # FC layer, e.g., 64 -> 128\n",
    "        self.dropout_fc = nn.Dropout(p=self.dropout_rate) # Dropout for the FC layer\n",
    "        self.out = nn.Linear(128, self.num_classes) # Output layer\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initializes weights for Conv2d and BatchNorm2d layers.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ResNetEmotion model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor. Expected shape [B, T, F] or [B, 1, T, F].\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape [B, num_classes].\n",
    "        \"\"\"\n",
    "        # Ensure input is 4D: [Batch, Channels, Time, Frequency]\n",
    "        if len(x.shape) == 3:  # If input is [B, T, F]\n",
    "            x = x.unsqueeze(1)  # Add channel dimension: [B, 1, T, F]\n",
    "        elif len(x.shape) != 4 or x.shape[1] != 1:\n",
    "             # This model assumes a single input channel (e.g., grayscale spectrogram)\n",
    "             # If multi-channel input, self.conv1's in_channels needs to change.\n",
    "            pass # Allow for [B, C, T, F] if C is already 1. Or raise error if C > 1 and not handled.\n",
    "            # For now, we assume if 4D, it's [B, 1, T, F] or user adapts conv1\n",
    "\n",
    "        # Initial convolution part\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x) # Shape: [B, base_channels, T/2, F/2]\n",
    "\n",
    "        # Residual blocks\n",
    "        x = self.res_block1(x) # Shape: [B, base_channels, T/2, F/2]\n",
    "        x = self.res_block2(x) # Shape: [B, base_channels*2, T/4, F/4]\n",
    "        # If more blocks were added:\n",
    "        # x = self.res_block3(x)\n",
    "        # x = self.res_block4(x)\n",
    "\n",
    "        # Pooling and Classification\n",
    "        x = self.adaptive_avg_pool(x) # Shape: [B, current_channels, 1, 1]\n",
    "        x = x.view(x.size(0), -1)    # Flatten: [B, current_channels]\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x) # Activation after the first FC layer\n",
    "        x = self.dropout_fc(x) # Apply dropout\n",
    "        x = self.out(x) # Final output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55832081",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training & Hyperparameter Tuning\n",
    "\n",
    "# Reasoning: Adam optimizer + moderate LR + stratified sampling + early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c03963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val)\n",
    "train_val_ds = EmotionDataset(X_train_val, y_train_val)\n",
    "train_ds = EmotionDataset(X_train, y_train)\n",
    "val_ds = EmotionDataset(X_val, y_val)\n",
    "test_ds = EmotionDataset(X_test, y_test)\n",
    "train_val_loader = DataLoader(train_val_ds, batch_size=16, shuffle=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "test_loader = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CNNEmotion(num_classes=len(le.classes_))\n",
    "model = TransformerEmotion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec2bb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Average Loss: 0.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Average Loss: 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed. Average Loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 completed. Average Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 completed. Average Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 completed. Average Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 completed. Average Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 completed. Average Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 completed. Average Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 completed. Average Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# # Training loop with tqdm progress bar\n",
    "# for epoch in range(40):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{40}\", leave=False)\n",
    "    \n",
    "#     for Xb, yb in progress_bar:\n",
    "#         Xb, yb = Xb.to(device), yb.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(model(Xb), yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "#         progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "#     scheduler.step()\n",
    "#     if (epoch + 1) % 4 == 0:\n",
    "#         print(f\"Epoch {epoch+1} completed. Average Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f08b7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=40, patience=8, model_path='best_model.pt'):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=correct/total)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved best model with Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40474cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning with Optuna\n",
    "def objective_cnn(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize CNN hyperparameters\n",
    "    \"\"\"\n",
    "    # Hyperparameters to optimize\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    \n",
    "    # Create data loaders with the suggested batch_size\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = CNNEmotion(num_classes=len(emotion_map), dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Train for a few epochs to see how it performs\n",
    "    trial_epochs = 15  # Reduced number of epochs for faster trials\n",
    "    patience = 5\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(trial_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update best accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report intermediate result\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning with Optuna\n",
    "def objective_rcnn(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize CNN hyperparameters\n",
    "    \"\"\"\n",
    "    # Hyperparameters to optimize\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    \n",
    "    # Create data loaders with the suggested batch_size\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = ResNetEmotion(num_classes=len(emotion_map), dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Train for a few epochs to see how it performs\n",
    "    trial_epochs = 15  # Reduced number of epochs for faster trials\n",
    "    patience = 5\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(trial_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update best accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report intermediate result\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae5e7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lstm(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize LSTM hyperparameters\n",
    "    \"\"\"\n",
    "    # Hyperparameters to optimize\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    gamma = trial.suggest_float('gamma', 0.5, 0.95)\n",
    "    step_size = trial.suggest_int('step_size', 3, 10)\n",
    "    \n",
    "    # Create data loaders with the suggested batch_size\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    # Determine input size based on data shape\n",
    "    sample_data = next(iter(train_loader))[0]\n",
    "    if len(sample_data.shape) == 4:  # If working with spectrograms\n",
    "        input_size = sample_data.shape[1] * sample_data.shape[3]  # Channels * Features\n",
    "    else:  # If working with MFCCs or similar\n",
    "        input_size = sample_data.shape[2]  # Feature dimension\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = LSTMEmotion(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=len(emotion_map),\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Train for a few epochs to see how it performs\n",
    "    trial_epochs = 15  # Reduced number of epochs for faster trials\n",
    "    patience = 5\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(trial_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update best accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report intermediate result\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            break\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_transformer(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize Transformer hyperparameters.\n",
    "    \"\"\"\n",
    "    # Hyperparameters to optimize\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True) # Adjusted lr range for transformers\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    gamma = trial.suggest_float('gamma', 0.7, 0.98) # Scheduler gamma\n",
    "    step_size = trial.suggest_int('step_size', 5, 15) # Scheduler step size\n",
    "\n",
    "    # Transformer specific hyperparameters\n",
    "    d_model = trial.suggest_categorical('d_model', [64, 128, 256])\n",
    "    \n",
    "\n",
    "    possible_nheads = [h for h in [2, 4, 8] if d_model % h == 0]\n",
    "    if not possible_nheads:\n",
    "        print(f\"Warning: No valid nhead for d_model={d_model}. Pruning or using default.\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    nhead = trial.suggest_categorical('nhead', possible_nheads)\n",
    "\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 2, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [256, 512, 1024, 2048])\n",
    "\n",
    "    # Create data loaders with the suggested batch_size\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # Determine input size based on data shape\n",
    "    sample_data = next(iter(train_loader))[0]\n",
    "    if len(sample_data.shape) == 4:  # If working with spectrograms\n",
    "        input_size = sample_data.shape[1] * sample_data.shape[3]  # Channels * Features\n",
    "    else:  # If working with MFCCs or similar\n",
    "        input_size = sample_data.shape[2]  # Feature dimension\n",
    "    \n",
    "    # Initialize the model\n",
    "    # Ensure MODEL_INPUT_SIZE and NUM_CLASSES are defined globally or passed\n",
    "    model = TransformerEmotion(\n",
    "        input_size=input_size, # This is C * F from the input data [B, C, T, F]\n",
    "        num_classes=len(emotion_map),\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    # Training parameters\n",
    "    trial_epochs = 15  # Reduced number of epochs for faster trials in HPO\n",
    "    patience = 5      # Early stopping patience\n",
    "    best_val_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(trial_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # inputs shape: [batch, channels, time_steps, features]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        # Update best validation accuracy and handle early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "            # Optionally save the best model for this trial (if needed later)\n",
    "            # torch.save(model.state_dict(), f\"best_model_trial_{trial.number}.pt\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report intermediate result to Optuna\n",
    "        # Optuna can use this to prune unpromising trials early\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} pruned at epoch {epoch+1}.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Early stopping based on patience\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}.\")\n",
    "            break\n",
    "            \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b7a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:47:30,678] A new study created in memory with name: no-name-8c92754f-7df7-41ef-9813-18f3dbe01495\n",
      "[I 2025-05-09 11:47:57,850] Trial 0 finished with value: 0.6416184971098265 and parameters: {'batch_size': 16, 'lr': 0.0001967279623158883, 'dropout_rate': 0.3906136197819599, 'gamma': 0.8825005449517268, 'step_size': 10}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:48:23,124] Trial 1 finished with value: 0.630057803468208 and parameters: {'batch_size': 16, 'lr': 0.0025105013763121833, 'dropout_rate': 0.26871587376873535, 'gamma': 0.8178351606784551, 'step_size': 8}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:48:36,663] Trial 2 finished with value: 0.6069364161849711 and parameters: {'batch_size': 32, 'lr': 0.0004304347670647763, 'dropout_rate': 0.40065048695741456, 'gamma': 0.6687256930400021, 'step_size': 7}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:48:50,923] Trial 3 finished with value: 0.5491329479768786 and parameters: {'batch_size': 64, 'lr': 0.003939282638123739, 'dropout_rate': 0.10211195629427433, 'gamma': 0.7079890810182429, 'step_size': 4}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:49:08,577] Trial 4 finished with value: 0.5375722543352601 and parameters: {'batch_size': 32, 'lr': 0.003471645502501275, 'dropout_rate': 0.48507150495442986, 'gamma': 0.9465924353513593, 'step_size': 6}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:49:13,777] Trial 5 pruned. \n",
      "[I 2025-05-09 11:49:16,637] Trial 6 pruned. \n",
      "[I 2025-05-09 11:49:17,850] Trial 7 pruned. \n",
      "[I 2025-05-09 11:49:21,396] Trial 8 pruned. \n",
      "[I 2025-05-09 11:49:22,387] Trial 9 pruned. \n",
      "[I 2025-05-09 11:49:36,072] Trial 10 pruned. \n",
      "[I 2025-05-09 11:49:48,150] Trial 11 pruned. \n",
      "[I 2025-05-09 11:50:13,642] Trial 12 finished with value: 0.630057803468208 and parameters: {'batch_size': 16, 'lr': 0.0004280777956003817, 'dropout_rate': 0.22225650256906226, 'gamma': 0.8668487096526902, 'step_size': 9}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:50:15,494] Trial 13 pruned. \n",
      "[I 2025-05-09 11:50:37,608] Trial 14 finished with value: 0.6416184971098265 and parameters: {'batch_size': 16, 'lr': 0.0007683152250980541, 'dropout_rate': 0.21252128297888057, 'gamma': 0.7710741023071337, 'step_size': 8}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:50:53,081] Trial 15 finished with value: 0.6358381502890174 and parameters: {'batch_size': 16, 'lr': 0.0008118412058773402, 'dropout_rate': 0.15285130870602917, 'gamma': 0.7557270060375002, 'step_size': 9}. Best is trial 0 with value: 0.6416184971098265.\n",
      "[I 2025-05-09 11:50:54,811] Trial 16 pruned. \n",
      "[I 2025-05-09 11:50:56,774] Trial 17 pruned. \n",
      "[I 2025-05-09 11:50:58,657] Trial 18 pruned. \n",
      "[I 2025-05-09 11:50:59,751] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN parameters: {'batch_size': 16, 'lr': 0.0001967279623158883, 'dropout_rate': 0.3906136197819599, 'gamma': 0.8825005449517268, 'step_size': 10}\n",
      "Best CNN validation accuracy: 0.6416\n"
     ]
    }
   ],
   "source": [
    "study_cnn = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_cnn.optimize(objective_cnn, n_trials=20)\n",
    "     \n",
    "best_params_cnn = study_cnn.best_params\n",
    "best_value_cnn = study_cnn.best_value\n",
    "        \n",
    "print(f\"Best CNN parameters: {best_params_cnn}\")\n",
    "print(f\"Best CNN validation accuracy: {best_value_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f4c5e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:20:27,165] A new study created in memory with name: no-name-844de630-7004-4708-9c48-df646d34c9a9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:23:16,894] Trial 0 finished with value: 0.3063583815028902 and parameters: {'batch_size': 16, 'lr': 0.00016743798813610125, 'dropout_rate': 0.46447803504989316, 'gamma': 0.5561336927668419, 'step_size': 3}. Best is trial 0 with value: 0.3063583815028902.\n",
      "[I 2025-05-09 13:26:07,042] Trial 1 finished with value: 0.3179190751445087 and parameters: {'batch_size': 32, 'lr': 0.006210290826806137, 'dropout_rate': 0.3962064301334963, 'gamma': 0.6946239674546281, 'step_size': 6}. Best is trial 1 with value: 0.3179190751445087.\n",
      "[I 2025-05-09 13:29:57,105] Trial 2 finished with value: 0.37572254335260113 and parameters: {'batch_size': 16, 'lr': 0.0015082516679554771, 'dropout_rate': 0.3387344320039753, 'gamma': 0.5826644079396117, 'step_size': 6}. Best is trial 2 with value: 0.37572254335260113.\n",
      "[I 2025-05-09 14:25:13,285] Trial 3 finished with value: 0.5028901734104047 and parameters: {'batch_size': 32, 'lr': 0.0005423749159850852, 'dropout_rate': 0.10972402274872205, 'gamma': 0.9421087553568287, 'step_size': 10}. Best is trial 3 with value: 0.5028901734104047.\n",
      "[I 2025-05-09 14:28:50,536] Trial 4 finished with value: 0.3699421965317919 and parameters: {'batch_size': 32, 'lr': 0.0040143641145261345, 'dropout_rate': 0.4550888978330417, 'gamma': 0.7834548423161475, 'step_size': 4}. Best is trial 3 with value: 0.5028901734104047.\n",
      "[I 2025-05-09 14:29:20,118] Trial 5 pruned. \n",
      "[I 2025-05-09 14:29:50,514] Trial 6 pruned. \n",
      "[I 2025-05-09 14:30:48,250] Trial 7 pruned. \n",
      "[I 2025-05-09 14:31:53,451] Trial 8 pruned. \n",
      "[I 2025-05-09 14:32:21,730] Trial 9 pruned. \n",
      "[I 2025-05-09 14:34:08,182] Trial 10 finished with value: 0.4046242774566474 and parameters: {'batch_size': 32, 'lr': 0.0018721990670042242, 'dropout_rate': 0.1253105446409492, 'gamma': 0.8116451354496241, 'step_size': 10}. Best is trial 3 with value: 0.5028901734104047.\n",
      "[I 2025-05-09 14:34:22,300] Trial 11 pruned. \n",
      "[I 2025-05-09 14:34:50,893] Trial 12 pruned. \n",
      "[I 2025-05-09 14:35:05,448] Trial 13 pruned. \n",
      "[I 2025-05-09 14:35:19,727] Trial 14 pruned. \n",
      "[I 2025-05-09 14:35:47,977] Trial 15 pruned. \n",
      "[I 2025-05-09 14:36:02,313] Trial 16 pruned. \n",
      "[I 2025-05-09 14:36:17,347] Trial 17 pruned. \n",
      "[I 2025-05-09 14:36:32,113] Trial 18 pruned. \n",
      "[I 2025-05-09 14:37:03,563] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Residual CNN parameters: {'batch_size': 32, 'lr': 0.0005423749159850852, 'dropout_rate': 0.10972402274872205, 'gamma': 0.9421087553568287, 'step_size': 10}\n",
      "Best Residual CNN validation accuracy: 0.5029\n"
     ]
    }
   ],
   "source": [
    "study_rcnn = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_rcnn.optimize(objective_rcnn, n_trials=20)\n",
    "     \n",
    "best_params_rcnn = study_rcnn.best_params\n",
    "best_value_rcnn = study_rcnn.best_value\n",
    "        \n",
    "print(f\"Best Residual CNN parameters: {best_params_rcnn}\")\n",
    "print(f\"Best Residual CNN validation accuracy: {best_value_rcnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7f1c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:05:47,997] A new study created in memory with name: no-name-93c91958-a4e8-4732-8f41-7eece4c443ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:05:58,665] Trial 0 finished with value: 0.31213872832369943 and parameters: {'batch_size': 64, 'lr': 0.00017536482363694292, 'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.2793969266387962, 'gamma': 0.7519723303952297, 'step_size': 9}. Best is trial 0 with value: 0.31213872832369943.\n",
      "[I 2025-05-09 12:06:44,838] Trial 1 finished with value: 0.5549132947976878 and parameters: {'batch_size': 64, 'lr': 0.0036734809321324444, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.31750121130546166, 'gamma': 0.8184033434338648, 'step_size': 4}. Best is trial 1 with value: 0.5549132947976878.\n",
      "[I 2025-05-09 12:07:45,779] Trial 2 finished with value: 0.5549132947976878 and parameters: {'batch_size': 64, 'lr': 0.0006593469655041047, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.2923945832977028, 'gamma': 0.5802755252556892, 'step_size': 10}. Best is trial 1 with value: 0.5549132947976878.\n",
      "[I 2025-05-09 12:08:07,468] Trial 3 finished with value: 0.6011560693641619 and parameters: {'batch_size': 32, 'lr': 0.006240483788863032, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.4186220070335792, 'gamma': 0.7089431116499806, 'step_size': 8}. Best is trial 3 with value: 0.6011560693641619.\n",
      "[I 2025-05-09 12:09:22,677] Trial 4 finished with value: 0.5838150289017341 and parameters: {'batch_size': 16, 'lr': 0.002102830150554783, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.1420831293763475, 'gamma': 0.8852807048514479, 'step_size': 8}. Best is trial 3 with value: 0.6011560693641619.\n",
      "[I 2025-05-09 12:09:24,753] Trial 5 pruned. \n",
      "[I 2025-05-09 12:09:38,364] Trial 6 pruned. \n",
      "[I 2025-05-09 12:09:41,267] Trial 7 pruned. \n",
      "[I 2025-05-09 12:09:44,511] Trial 8 pruned. \n",
      "[I 2025-05-09 12:09:46,467] Trial 9 pruned. \n",
      "[I 2025-05-09 12:09:47,921] Trial 10 pruned. \n",
      "[I 2025-05-09 12:10:46,187] Trial 11 pruned. \n",
      "[I 2025-05-09 12:10:59,074] Trial 12 pruned. \n",
      "[I 2025-05-09 12:11:18,191] Trial 13 pruned. \n",
      "[I 2025-05-09 12:11:35,267] Trial 14 pruned. \n",
      "[I 2025-05-09 12:12:03,142] Trial 15 finished with value: 0.6358381502890174 and parameters: {'batch_size': 16, 'lr': 0.0015988593067024631, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.16218936716097276, 'gamma': 0.9258406881492669, 'step_size': 6}. Best is trial 15 with value: 0.6358381502890174.\n",
      "[I 2025-05-09 12:12:05,730] Trial 16 pruned. \n",
      "[I 2025-05-09 12:12:07,325] Trial 17 pruned. \n",
      "[I 2025-05-09 12:12:36,267] Trial 18 pruned. \n",
      "[I 2025-05-09 12:12:43,514] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LSTM parameters: {'batch_size': 16, 'lr': 0.0015988593067024631, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.16218936716097276, 'gamma': 0.9258406881492669, 'step_size': 6}\n",
      "Best LSTM validation accuracy: 0.6358\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning LSTM model...\")\n",
    "study_lstm = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_lstm.optimize(objective_lstm, n_trials=20)\n",
    "        \n",
    "best_params_lstm = study_lstm.best_params\n",
    "best_value_lstm = study_lstm.best_value\n",
    "        \n",
    "print(f\"Best LSTM parameters: {best_params_lstm}\")\n",
    "print(f\"Best LSTM validation accuracy: {best_value_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "198514c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:54:51,497] A new study created in memory with name: no-name-9ef7774a-e4fd-4ba5-9b17-1807377855b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 1.5999, Train Acc: 0.2239 | Val Loss: 1.5861, Val Acc: 0.2254\n",
      "Epoch 2/15 - Train Loss: 1.6068, Train Acc: 0.2027 | Val Loss: 1.5832, Val Acc: 0.2254\n",
      "Epoch 3/15 - Train Loss: 1.6014, Train Acc: 0.1950 | Val Loss: 1.5857, Val Acc: 0.2197\n",
      "Epoch 4/15 - Train Loss: 1.5977, Train Acc: 0.2066 | Val Loss: 1.5862, Val Acc: 0.2197\n",
      "Epoch 5/15 - Train Loss: 1.5970, Train Acc: 0.2297 | Val Loss: 1.5853, Val Acc: 0.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:55:21,388] Trial 0 finished with value: 0.2254335260115607 and parameters: {'batch_size': 64, 'lr': 0.0008037259956403304, 'dropout_rate': 0.4461985772856851, 'gamma': 0.7850413730111595, 'step_size': 13, 'd_model': 64, 'nhead': 2, 'num_encoder_layers': 4, 'dim_feedforward': 2048}. Best is trial 0 with value: 0.2254335260115607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Train Loss: 1.6077, Train Acc: 0.2317 | Val Loss: 1.5879, Val Acc: 0.2254\n",
      "Early stopping at epoch 6 for trial 0.\n",
      "Epoch 1/15 - Train Loss: 1.6177, Train Acc: 0.1892 | Val Loss: 1.6093, Val Acc: 0.2254\n",
      "Epoch 2/15 - Train Loss: 1.6156, Train Acc: 0.2201 | Val Loss: 1.6031, Val Acc: 0.2254\n",
      "Epoch 3/15 - Train Loss: 1.6077, Train Acc: 0.2375 | Val Loss: 1.5986, Val Acc: 0.2254\n",
      "Epoch 4/15 - Train Loss: 1.5999, Train Acc: 0.2355 | Val Loss: 1.5949, Val Acc: 0.2254\n",
      "Epoch 5/15 - Train Loss: 1.6000, Train Acc: 0.2452 | Val Loss: 1.5916, Val Acc: 0.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:55:36,600] Trial 1 finished with value: 0.2254335260115607 and parameters: {'batch_size': 64, 'lr': 1.1175849178025742e-05, 'dropout_rate': 0.4047935618494282, 'gamma': 0.9265884840317343, 'step_size': 6, 'd_model': 64, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 2048}. Best is trial 0 with value: 0.2254335260115607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Train Loss: 1.5976, Train Acc: 0.2297 | Val Loss: 1.5887, Val Acc: 0.2254\n",
      "Early stopping at epoch 6 for trial 1.\n",
      "Epoch 1/15 - Train Loss: 1.6510, Train Acc: 0.2027 | Val Loss: 1.5938, Val Acc: 0.2197\n",
      "Epoch 2/15 - Train Loss: 1.6025, Train Acc: 0.2278 | Val Loss: 1.5987, Val Acc: 0.2254\n",
      "Epoch 3/15 - Train Loss: 1.5988, Train Acc: 0.1853 | Val Loss: 1.5940, Val Acc: 0.2197\n",
      "Epoch 4/15 - Train Loss: 1.5972, Train Acc: 0.2181 | Val Loss: 1.5834, Val Acc: 0.2197\n",
      "Epoch 5/15 - Train Loss: 1.5949, Train Acc: 0.2452 | Val Loss: 1.5827, Val Acc: 0.2254\n",
      "Epoch 6/15 - Train Loss: 1.5862, Train Acc: 0.2510 | Val Loss: 1.5813, Val Acc: 0.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:56:53,830] Trial 2 finished with value: 0.2254335260115607 and parameters: {'batch_size': 16, 'lr': 0.0002561134563649086, 'dropout_rate': 0.4723177642187102, 'gamma': 0.7087390875747439, 'step_size': 13, 'd_model': 256, 'nhead': 8, 'num_encoder_layers': 5, 'dim_feedforward': 2048}. Best is trial 0 with value: 0.2254335260115607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Train Loss: 1.5951, Train Acc: 0.2278 | Val Loss: 1.5814, Val Acc: 0.2254\n",
      "Early stopping at epoch 7 for trial 2.\n",
      "Epoch 1/15 - Train Loss: 1.6239, Train Acc: 0.2143 | Val Loss: 1.5859, Val Acc: 0.2197\n",
      "Epoch 2/15 - Train Loss: 1.5835, Train Acc: 0.2529 | Val Loss: 1.5550, Val Acc: 0.2775\n",
      "Epoch 3/15 - Train Loss: 1.5312, Train Acc: 0.2896 | Val Loss: 1.5103, Val Acc: 0.3121\n",
      "Epoch 4/15 - Train Loss: 1.4255, Train Acc: 0.3398 | Val Loss: 1.4081, Val Acc: 0.3873\n",
      "Epoch 5/15 - Train Loss: 1.3421, Train Acc: 0.4035 | Val Loss: 1.6693, Val Acc: 0.3121\n",
      "Epoch 6/15 - Train Loss: 1.3598, Train Acc: 0.4266 | Val Loss: 1.3390, Val Acc: 0.4277\n",
      "Epoch 7/15 - Train Loss: 1.1796, Train Acc: 0.5077 | Val Loss: 1.5294, Val Acc: 0.4162\n",
      "Epoch 8/15 - Train Loss: 1.0888, Train Acc: 0.5425 | Val Loss: 1.4118, Val Acc: 0.4220\n",
      "Epoch 9/15 - Train Loss: 1.0418, Train Acc: 0.5425 | Val Loss: 1.2302, Val Acc: 0.4855\n",
      "Epoch 10/15 - Train Loss: 0.8406, Train Acc: 0.6467 | Val Loss: 1.4932, Val Acc: 0.4509\n",
      "Epoch 11/15 - Train Loss: 0.7951, Train Acc: 0.6390 | Val Loss: 1.4059, Val Acc: 0.5260\n",
      "Epoch 12/15 - Train Loss: 0.6416, Train Acc: 0.7432 | Val Loss: 1.3725, Val Acc: 0.5434\n",
      "Epoch 13/15 - Train Loss: 0.4954, Train Acc: 0.7973 | Val Loss: 1.4981, Val Acc: 0.5376\n",
      "Epoch 14/15 - Train Loss: 0.3987, Train Acc: 0.8475 | Val Loss: 1.8163, Val Acc: 0.5838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:57:20,638] Trial 3 finished with value: 0.5838150289017341 and parameters: {'batch_size': 32, 'lr': 0.0009437696214549022, 'dropout_rate': 0.29922834096233236, 'gamma': 0.8449225569214032, 'step_size': 10, 'd_model': 128, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 1024}. Best is trial 3 with value: 0.5838150289017341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.5355, Train Acc: 0.7934 | Val Loss: 1.8118, Val Acc: 0.5145\n",
      "Epoch 1/15 - Train Loss: 1.6160, Train Acc: 0.2085 | Val Loss: 1.6072, Val Acc: 0.2023\n",
      "Epoch 2/15 - Train Loss: 1.6043, Train Acc: 0.2066 | Val Loss: 1.5972, Val Acc: 0.2023\n",
      "Epoch 3/15 - Train Loss: 1.6004, Train Acc: 0.2375 | Val Loss: 1.5901, Val Acc: 0.2832\n",
      "Epoch 4/15 - Train Loss: 1.5958, Train Acc: 0.2143 | Val Loss: 1.5852, Val Acc: 0.2948\n",
      "Epoch 5/15 - Train Loss: 1.5901, Train Acc: 0.2452 | Val Loss: 1.5810, Val Acc: 0.3121\n",
      "Epoch 6/15 - Train Loss: 1.5820, Train Acc: 0.2413 | Val Loss: 1.5779, Val Acc: 0.2370\n",
      "Epoch 7/15 - Train Loss: 1.5724, Train Acc: 0.2355 | Val Loss: 1.5747, Val Acc: 0.2254\n",
      "Epoch 8/15 - Train Loss: 1.5830, Train Acc: 0.2471 | Val Loss: 1.5714, Val Acc: 0.2717\n",
      "Epoch 9/15 - Train Loss: 1.5763, Train Acc: 0.2703 | Val Loss: 1.5689, Val Acc: 0.3179\n",
      "Epoch 10/15 - Train Loss: 1.5728, Train Acc: 0.2587 | Val Loss: 1.5666, Val Acc: 0.2890\n",
      "Epoch 11/15 - Train Loss: 1.5706, Train Acc: 0.2432 | Val Loss: 1.5645, Val Acc: 0.3295\n",
      "Epoch 12/15 - Train Loss: 1.5704, Train Acc: 0.2568 | Val Loss: 1.5628, Val Acc: 0.3584\n",
      "Epoch 13/15 - Train Loss: 1.5609, Train Acc: 0.2973 | Val Loss: 1.5607, Val Acc: 0.3526\n",
      "Epoch 14/15 - Train Loss: 1.5637, Train Acc: 0.2819 | Val Loss: 1.5590, Val Acc: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:57:41,528] Trial 4 finished with value: 0.3699421965317919 and parameters: {'batch_size': 64, 'lr': 9.298020781236625e-05, 'dropout_rate': 0.4635542388203598, 'gamma': 0.9142402720790773, 'step_size': 8, 'd_model': 64, 'nhead': 8, 'num_encoder_layers': 2, 'dim_feedforward': 256}. Best is trial 3 with value: 0.5838150289017341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 1.5556, Train Acc: 0.3263 | Val Loss: 1.5575, Val Acc: 0.3699\n",
      "Epoch 1/15 - Train Loss: 1.6036, Train Acc: 0.2375 | Val Loss: 1.5759, Val Acc: 0.3064\n",
      "Epoch 2/15 - Train Loss: 1.5837, Train Acc: 0.2201 | Val Loss: 1.5687, Val Acc: 0.2717\n",
      "Epoch 3/15 - Train Loss: 1.5796, Train Acc: 0.2355 | Val Loss: 1.5658, Val Acc: 0.2312\n",
      "Epoch 4/15 - Train Loss: 1.5763, Train Acc: 0.2375 | Val Loss: 1.5613, Val Acc: 0.2312\n",
      "Epoch 5/15 - Train Loss: 1.5670, Train Acc: 0.2529 | Val Loss: 1.5537, Val Acc: 0.2312\n",
      "Epoch 6/15 - Train Loss: 1.5548, Train Acc: 0.2664 | Val Loss: 1.5447, Val Acc: 0.3237\n",
      "Epoch 7/15 - Train Loss: 1.5468, Train Acc: 0.3243 | Val Loss: 1.5317, Val Acc: 0.3757\n",
      "Epoch 8/15 - Train Loss: 1.5345, Train Acc: 0.3282 | Val Loss: 1.5222, Val Acc: 0.3410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:57:59,046] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Train Loss: 1.5172, Train Acc: 0.3320 | Val Loss: 1.5151, Val Acc: 0.3237\n",
      "Trial 5 pruned at epoch 9.\n",
      "Epoch 1/15 - Train Loss: 1.6196, Train Acc: 0.2355 | Val Loss: 1.5751, Val Acc: 0.2312\n",
      "Epoch 2/15 - Train Loss: 1.5798, Train Acc: 0.2606 | Val Loss: 1.5339, Val Acc: 0.2948\n",
      "Epoch 3/15 - Train Loss: 1.4442, Train Acc: 0.3610 | Val Loss: 1.3508, Val Acc: 0.4393\n",
      "Epoch 4/15 - Train Loss: 1.2249, Train Acc: 0.4846 | Val Loss: 1.4957, Val Acc: 0.3353\n",
      "Epoch 5/15 - Train Loss: 1.1397, Train Acc: 0.5019 | Val Loss: 1.4754, Val Acc: 0.3988\n",
      "Epoch 6/15 - Train Loss: 1.0486, Train Acc: 0.5367 | Val Loss: 1.4544, Val Acc: 0.4393\n",
      "Epoch 7/15 - Train Loss: 0.8245, Train Acc: 0.6390 | Val Loss: 1.3101, Val Acc: 0.4913\n",
      "Epoch 8/15 - Train Loss: 0.7223, Train Acc: 0.6622 | Val Loss: 1.8183, Val Acc: 0.4220\n",
      "Epoch 9/15 - Train Loss: 0.7269, Train Acc: 0.6757 | Val Loss: 1.5640, Val Acc: 0.4509\n",
      "Epoch 10/15 - Train Loss: 0.6489, Train Acc: 0.7375 | Val Loss: 1.3139, Val Acc: 0.5260\n",
      "Epoch 11/15 - Train Loss: 0.4362, Train Acc: 0.8301 | Val Loss: 1.5094, Val Acc: 0.5318\n",
      "Epoch 12/15 - Train Loss: 0.4155, Train Acc: 0.8263 | Val Loss: 1.6086, Val Acc: 0.5318\n",
      "Epoch 13/15 - Train Loss: 0.3549, Train Acc: 0.8533 | Val Loss: 2.1234, Val Acc: 0.5376\n",
      "Epoch 14/15 - Train Loss: 0.3488, Train Acc: 0.8629 | Val Loss: 1.5246, Val Acc: 0.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:58:42,330] Trial 6 finished with value: 0.5780346820809249 and parameters: {'batch_size': 16, 'lr': 0.0007540057085850046, 'dropout_rate': 0.23395831126177732, 'gamma': 0.8676215095545362, 'step_size': 8, 'd_model': 128, 'nhead': 4, 'num_encoder_layers': 4, 'dim_feedforward': 256}. Best is trial 3 with value: 0.5838150289017341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.2044, Train Acc: 0.9286 | Val Loss: 1.8333, Val Acc: 0.5780\n",
      "Epoch 1/15 - Train Loss: 1.5919, Train Acc: 0.2568 | Val Loss: 1.5556, Val Acc: 0.2659\n",
      "Epoch 2/15 - Train Loss: 1.5553, Train Acc: 0.2741 | Val Loss: 1.5216, Val Acc: 0.3179\n",
      "Epoch 3/15 - Train Loss: 1.4963, Train Acc: 0.3552 | Val Loss: 1.5254, Val Acc: 0.2717\n",
      "Epoch 4/15 - Train Loss: 1.4345, Train Acc: 0.3803 | Val Loss: 1.4063, Val Acc: 0.4104\n",
      "Epoch 5/15 - Train Loss: 1.3536, Train Acc: 0.4170 | Val Loss: 1.3506, Val Acc: 0.3931\n",
      "Epoch 6/15 - Train Loss: 1.3019, Train Acc: 0.4691 | Val Loss: 1.4586, Val Acc: 0.3584\n",
      "Epoch 7/15 - Train Loss: 1.2039, Train Acc: 0.5000 | Val Loss: 1.2820, Val Acc: 0.4220\n",
      "Epoch 8/15 - Train Loss: 1.1621, Train Acc: 0.5019 | Val Loss: 1.2669, Val Acc: 0.4335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:59:17,679] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Train Loss: 1.0723, Train Acc: 0.5598 | Val Loss: 1.2658, Val Acc: 0.4335\n",
      "Trial 7 pruned at epoch 9.\n",
      "Epoch 1/15 - Train Loss: 1.5983, Train Acc: 0.2066 | Val Loss: 1.5754, Val Acc: 0.2890\n",
      "Epoch 2/15 - Train Loss: 1.5818, Train Acc: 0.2510 | Val Loss: 1.5673, Val Acc: 0.2543\n",
      "Epoch 3/15 - Train Loss: 1.5730, Train Acc: 0.2625 | Val Loss: 1.5595, Val Acc: 0.3295\n",
      "Epoch 4/15 - Train Loss: 1.5583, Train Acc: 0.2915 | Val Loss: 1.5472, Val Acc: 0.3757\n",
      "Epoch 5/15 - Train Loss: 1.5392, Train Acc: 0.3224 | Val Loss: 1.5333, Val Acc: 0.3757\n",
      "Epoch 6/15 - Train Loss: 1.5282, Train Acc: 0.3398 | Val Loss: 1.5160, Val Acc: 0.3410\n",
      "Epoch 7/15 - Train Loss: 1.5069, Train Acc: 0.3475 | Val Loss: 1.4947, Val Acc: 0.3468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:59:29,701] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Train Loss: 1.4781, Train Acc: 0.3649 | Val Loss: 1.4717, Val Acc: 0.3873\n",
      "Trial 8 pruned at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:59:33,744] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 1.6058, Train Acc: 0.2452 | Val Loss: 1.6187, Val Acc: 0.2197\n",
      "Trial 9 pruned at epoch 1.\n",
      "Epoch 1/15 - Train Loss: 1.5957, Train Acc: 0.1873 | Val Loss: 1.5815, Val Acc: 0.2370\n",
      "Epoch 2/15 - Train Loss: 1.5807, Train Acc: 0.2645 | Val Loss: 1.5726, Val Acc: 0.3295\n",
      "Epoch 3/15 - Train Loss: 1.5727, Train Acc: 0.2587 | Val Loss: 1.5673, Val Acc: 0.2486\n",
      "Epoch 4/15 - Train Loss: 1.5647, Train Acc: 0.3069 | Val Loss: 1.5608, Val Acc: 0.3295\n",
      "Epoch 5/15 - Train Loss: 1.5544, Train Acc: 0.3842 | Val Loss: 1.5536, Val Acc: 0.3931\n",
      "Epoch 6/15 - Train Loss: 1.5466, Train Acc: 0.3726 | Val Loss: 1.5471, Val Acc: 0.3873\n",
      "Epoch 7/15 - Train Loss: 1.5375, Train Acc: 0.3784 | Val Loss: 1.5394, Val Acc: 0.4335\n",
      "Epoch 8/15 - Train Loss: 1.5256, Train Acc: 0.3900 | Val Loss: 1.5320, Val Acc: 0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:59:59,727] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Train Loss: 1.5162, Train Acc: 0.4151 | Val Loss: 1.5238, Val Acc: 0.3873\n",
      "Trial 10 pruned at epoch 9.\n",
      "Epoch 1/15 - Train Loss: 1.6450, Train Acc: 0.2027 | Val Loss: 1.5879, Val Acc: 0.2254\n",
      "Epoch 2/15 - Train Loss: 1.6049, Train Acc: 0.2066 | Val Loss: 1.5824, Val Acc: 0.2197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:00:09,494] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Train Loss: 1.5618, Train Acc: 0.2761 | Val Loss: 1.6412, Val Acc: 0.2254\n",
      "Trial 11 pruned at epoch 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:00:12,877] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 1.6091, Train Acc: 0.2394 | Val Loss: 1.5938, Val Acc: 0.2197\n",
      "Trial 12 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:00:15,906] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 1.6173, Train Acc: 0.2085 | Val Loss: 1.5704, Val Acc: 0.2197\n",
      "Trial 13 pruned at epoch 1.\n",
      "Epoch 1/15 - Train Loss: 1.6371, Train Acc: 0.2297 | Val Loss: 1.5934, Val Acc: 0.2254\n",
      "Epoch 2/15 - Train Loss: 1.6183, Train Acc: 0.2124 | Val Loss: 1.5617, Val Acc: 0.3295\n",
      "Epoch 3/15 - Train Loss: 1.5479, Train Acc: 0.3050 | Val Loss: 1.5194, Val Acc: 0.3237\n",
      "Epoch 4/15 - Train Loss: 1.4542, Train Acc: 0.3340 | Val Loss: 1.5327, Val Acc: 0.2948\n",
      "Epoch 5/15 - Train Loss: 1.3932, Train Acc: 0.3610 | Val Loss: 1.4959, Val Acc: 0.3468\n",
      "Epoch 6/15 - Train Loss: 1.3185, Train Acc: 0.4189 | Val Loss: 1.3614, Val Acc: 0.4104\n",
      "Epoch 7/15 - Train Loss: 1.2151, Train Acc: 0.4266 | Val Loss: 1.4903, Val Acc: 0.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:00:57,074] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Train Loss: 1.2218, Train Acc: 0.4942 | Val Loss: 1.5546, Val Acc: 0.3988\n",
      "Trial 14 pruned at epoch 8.\n",
      "Epoch 1/15 - Train Loss: 1.6029, Train Acc: 0.2124 | Val Loss: 1.5681, Val Acc: 0.2717\n",
      "Epoch 2/15 - Train Loss: 1.5719, Train Acc: 0.3069 | Val Loss: 1.5560, Val Acc: 0.2832\n",
      "Epoch 3/15 - Train Loss: 1.5481, Train Acc: 0.3127 | Val Loss: 1.5386, Val Acc: 0.3410\n",
      "Epoch 4/15 - Train Loss: 1.5312, Train Acc: 0.3282 | Val Loss: 1.5154, Val Acc: 0.3468\n",
      "Epoch 5/15 - Train Loss: 1.4921, Train Acc: 0.3610 | Val Loss: 1.4887, Val Acc: 0.3295\n",
      "Epoch 6/15 - Train Loss: 1.4577, Train Acc: 0.3668 | Val Loss: 1.4551, Val Acc: 0.3699\n",
      "Epoch 7/15 - Train Loss: 1.4087, Train Acc: 0.4286 | Val Loss: 1.4152, Val Acc: 0.4046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:01:32,252] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Train Loss: 1.3661, Train Acc: 0.4402 | Val Loss: 1.4304, Val Acc: 0.3699\n",
      "Trial 15 pruned at epoch 8.\n",
      "Epoch 1/15 - Train Loss: 1.6245, Train Acc: 0.1892 | Val Loss: 1.5645, Val Acc: 0.2717\n",
      "Epoch 2/15 - Train Loss: 1.5637, Train Acc: 0.2587 | Val Loss: 1.5230, Val Acc: 0.2832\n",
      "Epoch 3/15 - Train Loss: 1.4736, Train Acc: 0.3378 | Val Loss: 1.4242, Val Acc: 0.4104\n",
      "Epoch 4/15 - Train Loss: 1.3087, Train Acc: 0.4691 | Val Loss: 1.3030, Val Acc: 0.4104\n",
      "Epoch 5/15 - Train Loss: 1.2557, Train Acc: 0.4653 | Val Loss: 1.3859, Val Acc: 0.4046\n",
      "Epoch 6/15 - Train Loss: 1.1676, Train Acc: 0.4942 | Val Loss: 1.3043, Val Acc: 0.4046\n",
      "Epoch 7/15 - Train Loss: 0.9484, Train Acc: 0.6236 | Val Loss: 1.2228, Val Acc: 0.5029\n",
      "Epoch 8/15 - Train Loss: 0.7655, Train Acc: 0.6931 | Val Loss: 1.2715, Val Acc: 0.5607\n",
      "Epoch 9/15 - Train Loss: 0.5906, Train Acc: 0.7780 | Val Loss: 1.3867, Val Acc: 0.5780\n",
      "Epoch 10/15 - Train Loss: 0.4253, Train Acc: 0.8494 | Val Loss: 1.5034, Val Acc: 0.5087\n",
      "Epoch 11/15 - Train Loss: 0.4217, Train Acc: 0.8417 | Val Loss: 2.2315, Val Acc: 0.4624\n",
      "Epoch 12/15 - Train Loss: 0.6923, Train Acc: 0.7722 | Val Loss: 1.3274, Val Acc: 0.5896\n",
      "Epoch 13/15 - Train Loss: 0.2497, Train Acc: 0.9286 | Val Loss: 1.4196, Val Acc: 0.6301\n",
      "Epoch 14/15 - Train Loss: 0.1961, Train Acc: 0.9421 | Val Loss: 1.3967, Val Acc: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:02:36,695] Trial 16 finished with value: 0.630057803468208 and parameters: {'batch_size': 16, 'lr': 0.0002573623327473515, 'dropout_rate': 0.3015664111677139, 'gamma': 0.969769753507186, 'step_size': 12, 'd_model': 256, 'nhead': 4, 'num_encoder_layers': 3, 'dim_feedforward': 1024}. Best is trial 16 with value: 0.630057803468208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.1750, Train Acc: 0.9440 | Val Loss: 1.7048, Val Acc: 0.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:02:40,606] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 1.5945, Train Acc: 0.2471 | Val Loss: 1.5817, Val Acc: 0.2197\n",
      "Trial 17 pruned at epoch 1.\n",
      "Epoch 1/15 - Train Loss: 1.5885, Train Acc: 0.2490 | Val Loss: 1.5622, Val Acc: 0.2312\n",
      "Epoch 2/15 - Train Loss: 1.5623, Train Acc: 0.2954 | Val Loss: 1.5363, Val Acc: 0.3064\n",
      "Epoch 3/15 - Train Loss: 1.4665, Train Acc: 0.3649 | Val Loss: 1.4292, Val Acc: 0.3699\n",
      "Epoch 4/15 - Train Loss: 1.3418, Train Acc: 0.4015 | Val Loss: 1.3316, Val Acc: 0.4277\n",
      "Epoch 5/15 - Train Loss: 1.2679, Train Acc: 0.4382 | Val Loss: 1.3633, Val Acc: 0.4451\n",
      "Epoch 6/15 - Train Loss: 1.1275, Train Acc: 0.5386 | Val Loss: 1.4049, Val Acc: 0.3988\n",
      "Epoch 7/15 - Train Loss: 0.9969, Train Acc: 0.5888 | Val Loss: 1.2295, Val Acc: 0.5318\n",
      "Epoch 8/15 - Train Loss: 0.8738, Train Acc: 0.6544 | Val Loss: 1.3453, Val Acc: 0.4855\n",
      "Epoch 9/15 - Train Loss: 0.7222, Train Acc: 0.7297 | Val Loss: 1.1855, Val Acc: 0.5376\n",
      "Epoch 10/15 - Train Loss: 0.5330, Train Acc: 0.7992 | Val Loss: 1.2303, Val Acc: 0.5780\n",
      "Epoch 11/15 - Train Loss: 0.3574, Train Acc: 0.8958 | Val Loss: 1.2040, Val Acc: 0.5780\n",
      "Epoch 12/15 - Train Loss: 0.2723, Train Acc: 0.8996 | Val Loss: 1.5538, Val Acc: 0.5780\n",
      "Epoch 13/15 - Train Loss: 0.1590, Train Acc: 0.9479 | Val Loss: 1.6141, Val Acc: 0.5780\n",
      "Epoch 14/15 - Train Loss: 0.1107, Train Acc: 0.9807 | Val Loss: 1.4543, Val Acc: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:03:23,956] Trial 18 finished with value: 0.6358381502890174 and parameters: {'batch_size': 16, 'lr': 0.0001915902878499993, 'dropout_rate': 0.29237829199959525, 'gamma': 0.9713242733839934, 'step_size': 15, 'd_model': 256, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 1024}. Best is trial 18 with value: 0.6358381502890174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.0770, Train Acc: 0.9807 | Val Loss: 1.7871, Val Acc: 0.6358\n",
      "Epoch 1/15 - Train Loss: 1.5993, Train Acc: 0.2297 | Val Loss: 1.5417, Val Acc: 0.3757\n",
      "Epoch 2/15 - Train Loss: 1.5396, Train Acc: 0.2973 | Val Loss: 1.4917, Val Acc: 0.2948\n",
      "Epoch 3/15 - Train Loss: 1.4446, Train Acc: 0.3629 | Val Loss: 1.4138, Val Acc: 0.3988\n",
      "Epoch 4/15 - Train Loss: 1.2715, Train Acc: 0.4633 | Val Loss: 1.5554, Val Acc: 0.3179\n",
      "Epoch 5/15 - Train Loss: 1.1503, Train Acc: 0.5251 | Val Loss: 1.2632, Val Acc: 0.4451\n",
      "Epoch 6/15 - Train Loss: 1.0768, Train Acc: 0.5463 | Val Loss: 1.4984, Val Acc: 0.4509\n",
      "Epoch 7/15 - Train Loss: 0.9051, Train Acc: 0.6486 | Val Loss: 1.3690, Val Acc: 0.4624\n",
      "Epoch 8/15 - Train Loss: 0.7677, Train Acc: 0.6911 | Val Loss: 1.2196, Val Acc: 0.5780\n",
      "Epoch 9/15 - Train Loss: 0.6595, Train Acc: 0.7452 | Val Loss: 1.3299, Val Acc: 0.5549\n",
      "Epoch 10/15 - Train Loss: 0.6584, Train Acc: 0.7568 | Val Loss: 1.3288, Val Acc: 0.5318\n",
      "Epoch 11/15 - Train Loss: 0.4561, Train Acc: 0.8378 | Val Loss: 1.2234, Val Acc: 0.6127\n",
      "Epoch 12/15 - Train Loss: 0.2743, Train Acc: 0.8977 | Val Loss: 1.2918, Val Acc: 0.5896\n",
      "Epoch 13/15 - Train Loss: 0.1928, Train Acc: 0.9382 | Val Loss: 1.3093, Val Acc: 0.6301\n",
      "Epoch 14/15 - Train Loss: 0.1243, Train Acc: 0.9633 | Val Loss: 1.7854, Val Acc: 0.5665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 13:04:28,492] Trial 19 finished with value: 0.630057803468208 and parameters: {'batch_size': 16, 'lr': 0.00019775714849700512, 'dropout_rate': 0.3103624009447146, 'gamma': 0.9765054951777933, 'step_size': 15, 'd_model': 256, 'nhead': 4, 'num_encoder_layers': 3, 'dim_feedforward': 1024}. Best is trial 18 with value: 0.6358381502890174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Train Loss: 0.1269, Train Acc: 0.9537 | Val Loss: 1.6374, Val Acc: 0.6012\n",
      "Best CNN parameters: {'batch_size': 16, 'lr': 0.0001915902878499993, 'dropout_rate': 0.29237829199959525, 'gamma': 0.9713242733839934, 'step_size': 15, 'd_model': 256, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 1024}\n",
      "Best CNN validation accuracy: 0.6358\n"
     ]
    }
   ],
   "source": [
    "study_transformer = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_transformer.optimize(objective_transformer, n_trials=20)\n",
    "     \n",
    "best_params_transformer = study_transformer.best_params\n",
    "best_value_transformer = study_transformer.best_value\n",
    "        \n",
    "print(f\"Best CNN parameters: {best_params_transformer}\")\n",
    "print(f\"Best CNN validation accuracy: {best_value_transformer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f3aa5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f41e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.68      0.79      0.73        38\n",
      "     excited       0.76      0.58      0.66        38\n",
      "       happy       0.56      0.59      0.57        39\n",
      "     neutral       0.78      0.74      0.76        19\n",
      "         sad       0.68      0.72      0.70        39\n",
      "\n",
      "    accuracy                           0.68       173\n",
      "   macro avg       0.69      0.68      0.68       173\n",
      "weighted avg       0.68      0.68      0.68       173\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        outputs = model(Xb)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(yb.numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db81ba",
   "metadata": {},
   "source": [
    "## 10. Deployment Considerations\n",
    "# Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7699ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved at best_model/emotion_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dir = \"best_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"emotion_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ Model saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ae903",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f66e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️ Recording...\n",
      "✅ Saved to data/live_input.wav\n"
     ]
    }
   ],
   "source": [
    "# import sounddevice as sd\n",
    "# from scipy.io.wavfile import write\n",
    "\n",
    "# fs = 16000  # Sample rate\n",
    "# duration = 3  # Seconds\n",
    "# print(\"🎙️ Recording...\")\n",
    "# audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "# sd.wait()\n",
    "# write(\"data/live_input.wav\", fs, audio)\n",
    "# print(\"✅ Saved to data/live_input.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c33f71",
   "metadata": {},
   "source": [
    "2. 🧪 Preprocess the Recorded Audio\n",
    "Use the same feature extraction logic you used during training. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6c008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_live_audio(path, device, target_shape=(94, 92)):\n",
    "    y, sr = librosa.load(path, sr=16000)\n",
    "    y = librosa.util.fix_length(y, size=sr * 3)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=target_shape[0])\n",
    "\n",
    "    # Pad or trim time axis to match target_shape[1]\n",
    "    curr_frames = mfcc.shape[1]\n",
    "    target_frames = target_shape[1]\n",
    "\n",
    "    if curr_frames < target_frames:\n",
    "        pad_width = target_frames - curr_frames\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    elif curr_frames > target_frames:\n",
    "        mfcc = mfcc[:, :target_frames]\n",
    "\n",
    "    mfcc_tensor = torch.tensor(mfcc).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, 94, 92]\n",
    "    print(f\"MFCC shape: {mfcc_tensor.shape}\")\n",
    "    return mfcc_tensor.to(device).float()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cf40c",
   "metadata": {},
   "source": [
    "3. 🔍 Load Model and Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2c6f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: torch.Size([1, 1, 94, 92])\n",
      "🧠 Predicted Emotion: excited\n"
     ]
    }
   ],
   "source": [
    "model = CNNEmotion(num_classes=5)  # or 4 if not including \"excited\"\n",
    "model.load_state_dict(torch.load(\"best_model/emotion_model.pth\"))\n",
    "model.eval().to(device)\n",
    "\n",
    "input_tensor = preprocess_live_audio(\"data/live_input.wav\", device)\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "emotions = ['neutral', 'happy', 'sad', 'angry', 'excited']\n",
    "print(f\"🧠 Predicted Emotion: {emotions[pred_idx]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d9f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
